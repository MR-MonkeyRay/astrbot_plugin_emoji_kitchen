import os
import json
import asyncio
import hashlib
import time
from pathlib import Path
from collections import OrderedDict

import regex
import aiohttp

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, StarTools, register
from astrbot.api import logger

# ===== å¸¸é‡ =====
HARDCODED_DATES = [
    "20251029", "20250501", "20250430", "20250204", "20250130",
    "20241023", "20241021", "20240610", "20240530", "20240214",
    "20240206", "20231128", "20231113", "20230821", "20230818",
    "20230803", "20230426", "20230418", "20230301", "20230216",
    "20230127", "20230126", "20221107", "20221101", "20220815",
    "20220506", "20220406", "20220203", "20220110", "20211115",
    "20210831", "20210521", "20210218", "20201001",
]

EMOJI_PATTERN = regex.compile(
    r'\p{Extended_Pictographic}'
    r'(?:\ufe0f|\ufe0e)?'
    r'(?:[\U0001F3FB-\U0001F3FF])?'
    r'(?:\u200d\p{Extended_Pictographic}'
    r'(?:\ufe0f|\ufe0e)?'
    r'(?:[\U0001F3FB-\U0001F3FF])?)*',
    regex.UNICODE
)

# ===== å¼‚å¸¸ =====
class RateLimitError(Exception):
    """CDN é™æµå¼‚å¸¸ï¼Œéœ€è¦ç«‹å³åœæ­¢æ¢æµ‹"""
    pass

# ===== å·¥å…·å‡½æ•° =====
def emoji_to_codepoint(emoji_str: str) -> str:
    """å°† emoji å­—ç¬¦ä¸²è½¬ä¸º codepoint æ ¼å¼ã€‚
    ğŸ˜€ â†’ '1f600', â¤ï¸ â†’ '2764-fe0f'
    """
    return "-".join(f"{ord(c):x}" for c in emoji_str)

def codepoint_to_url_segment(cp: str) -> str:
    """å°† codepoint è½¬ä¸º URL è·¯å¾„æ®µã€‚
    '1f600' â†’ 'u1f600', '2764-fe0f' â†’ 'u2764-ufe0f'
    """
    return "-".join(f"u{part}" for part in cp.split("-"))

def make_cache_key(cp1: str, cp2: str) -> str:
    """ç”Ÿæˆæ’åºåçš„ç¼“å­˜ keyã€‚ä¿è¯ A+B ä¸ B+A å‘½ä¸­åŒä¸€ç¼“å­˜ã€‚"""
    return "_".join(sorted([cp1, cp2]))

def _parse_combinations(data: dict) -> dict[str, str]:
    """ä»å…ƒæ•°æ® JSON ä¸­è§£æç»„åˆç´¢å¼•ã€‚è¿”å› {partner_cp: date}"""
    index_entry = {}
    combinations = data.get("combinations", {}) if isinstance(data, dict) else {}
    for partner_cp, combo_list in combinations.items():
        if not isinstance(combo_list, list) or not combo_list:
            continue
        chosen = None
        for item in combo_list:
            if not isinstance(item, dict):
                continue
            if item.get("isLatest"):
                chosen = item
                break
        if chosen is None:
            chosen = combo_list[0] if isinstance(combo_list[0], dict) else None
        if chosen is None:
            continue
        date = chosen.get("date", "")
        if date:
            index_entry[partner_cp] = date
    return index_entry

# ===== æ’ä»¶ä¸»ç±» =====
@register("astrbot_plugin_emoji_kitchen", "monkeyray", "å‘é€ä¸¤ä¸ª emoji è‡ªåŠ¨åˆæˆ Google Emoji Kitchen å›¾ç‰‡", "1.1.0")
class EmojiKitchenPlugin(Star):
    _MAX_LOCKS = 1024
    _CONCURRENT_LIMIT = 4

    def __init__(self, context: Context, config: dict | None = None):
        super().__init__(context)
        self.config = config or {}
        self.data_dir: Path = Path("")
        self.cache_dir: Path = Path("")
        self.notfound_dir: Path = Path("")
        self.dates_cache_path: Path = Path("")
        self.date_list: list[str] = []
        self._locks: OrderedDict[str, asyncio.Lock] = OrderedDict()
        self._global_lock = asyncio.Lock()
        self._session_lock = asyncio.Lock()
        self._session: aiohttp.ClientSession | None = None
        self._semaphore: asyncio.Semaphore | None = None
        self._update_task: asyncio.Task | None = None
        self.metadata_dir: Path = Path("")
        self.metadata_index: dict[str, dict[str, str]] = {}  # {emoji_cp: {partner_cp: date}}

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–ï¼šåˆ›å»ºç›®å½•ã€åŠ è½½é…ç½®ã€å¯åŠ¨æ—¥æœŸæ›´æ–°"""
        self.data_dir = StarTools.get_data_dir()
        self.cache_dir = self.data_dir / "cache"
        self.notfound_dir = self.data_dir / "notfound"
        self.dates_cache_path = self.data_dir / "dates_cache.json"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.notfound_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_dir = self.data_dir / "metadata"
        self.metadata_dir.mkdir(parents=True, exist_ok=True)
        self._load_metadata_index()
        # åˆå§‹åŒ– semaphore
        self._semaphore = asyncio.Semaphore(self._CONCURRENT_LIMIT)
        # é¢„åˆ›å»º session
        await self._ensure_session()
        # åŠ è½½æ—¥æœŸåˆ—è¡¨
        self._load_date_list()
        # å¼‚æ­¥æ›´æ–°è¿œç¨‹æ—¥æœŸï¼ˆä¸é˜»å¡åˆå§‹åŒ–ï¼‰
        self._update_task = asyncio.create_task(self._update_dates_from_remote())
        logger.info(f"Emoji Kitchen æ’ä»¶åˆå§‹åŒ–å®Œæˆï¼Œæ—¥æœŸåˆ—è¡¨: {len(self.date_list)} ä¸ª")

    async def _ensure_session(self) -> aiohttp.ClientSession:
        """ç¡®ä¿ session å•ä¾‹å­˜åœ¨ï¼ˆåŒé‡æ£€æŸ¥é”ï¼‰"""
        if self._session and not self._session.closed:
            return self._session
        async with self._session_lock:
            if self._session and not self._session.closed:
                return self._session
            self._session = aiohttp.ClientSession(
                connector=aiohttp.TCPConnector(limit=20, ttl_dns_cache=300)
            )
            return self._session

    def _get_config(self, key: str, default=None):
        """ä»æ’ä»¶é…ç½®ä¸­è·å–å€¼"""
        if self.config and key in self.config:
            return self.config[key]
        return default

    def _get_config_int(self, key: str, default: int) -> int:
        """ä»æ’ä»¶é…ç½®ä¸­è·å–æ•´æ•°å€¼ï¼Œç±»å‹å¼‚å¸¸æ—¶è¿”å›é»˜è®¤å€¼"""
        val = self._get_config(key, default)
        try:
            return int(val)
        except (TypeError, ValueError):
            return default

    def _resolve_cdn_url(self) -> str:
        """è§£æå®é™…çš„ CDN åœ°å€ï¼šä¼˜å…ˆ cdn_source é¢„è®¾ï¼Œè‡ªå®šä¹‰æ—¶ç”¨ cdn_url"""
        source = str(self._get_config("cdn_source", "") or "")
        if source.startswith("www.gstatic.cn"):
            return "https://www.gstatic.cn"
        elif source.startswith("www.gstatic.com"):
            return "https://www.gstatic.com"
        elif source == "è‡ªå®šä¹‰":
            custom = str(self._get_config("cdn_url", "") or "").strip().rstrip("/")
            if custom:
                return custom
        elif not source:
            # å…¼å®¹æ—§é…ç½®ï¼šcdn_source ä¸ºç©ºæ—¶æ£€æŸ¥æ—§çš„ cdn_url å­—æ®µ
            legacy = str(self._get_config("cdn_url", "") or "").strip().rstrip("/")
            if legacy:
                return legacy
        # é»˜è®¤ï¼ˆç©ºæˆ–æœªè¯†åˆ«ï¼‰ï¼šè¿”å› gstatic.cn
        return "https://www.gstatic.cn"

    def _resolve_github_proxy(self) -> str:
        """è§£æå®é™…çš„ GitHub ä»£ç†åœ°å€ï¼šè¿”å›ä»£ç† URL æˆ–ç©ºå­—ç¬¦ä¸²ï¼ˆç›´è¿ï¼‰"""
        source = str(self._get_config("github_proxy_source", "") or "")
        if source.startswith("ghfast.top"):
            return "https://ghfast.top"
        elif source.startswith("gh-proxy.com"):
            return "https://gh-proxy.com"
        elif source == "è‡ªå®šä¹‰":
            custom = str(self._get_config("github_proxy", "") or "").strip().rstrip("/")
            if custom:
                return custom
        elif source == "ä¸ä½¿ç”¨ä»£ç†":
            return ""
        elif not source:
            # å…¼å®¹æ—§é…ç½®ï¼šgithub_proxy_source ä¸ºç©ºæ—¶æ£€æŸ¥æ—§çš„ github_proxy å­—æ®µ
            legacy = str(self._get_config("github_proxy", "") or "").strip().rstrip("/")
            if legacy:
                return legacy
        # é»˜è®¤ï¼ˆç©ºæˆ–æœªè¯†åˆ«ï¼‰ï¼šè¿”å› ghfast.top
        return "https://ghfast.top"

    def _get_cached_image(self, cache_key: str) -> str | None:
        """æ£€æŸ¥ç¼“å­˜å›¾ç‰‡æ˜¯å¦å­˜åœ¨ï¼Œè¿”å›è·¯å¾„æˆ– None"""
        path = self.cache_dir / f"{cache_key}.png"
        if path.exists():
            return str(path)
        return None

    def _is_notfound(self, cache_key: str) -> bool:
        """æ£€æŸ¥ notfound æ ‡è®°æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        path = self.notfound_dir / f"{cache_key}.json"
        if not path.exists():
            return False
        try:
            with open(path, encoding="utf-8") as f:
                data = json.load(f)
            # æ£€æŸ¥è¿‡æœŸ
            expire_days = self._get_config_int("notfound_expire_days", 7)
            if time.time() - data.get("timestamp", 0) > expire_days * 86400:
                path.unlink(missing_ok=True)
                return False
            # æ£€æŸ¥æ—¥æœŸåˆ—è¡¨æ˜¯å¦å˜åŒ–
            current_hash = self._get_date_list_hash()
            if data.get("date_list_hash") != current_hash:
                path.unlink(missing_ok=True)
                return False
            return True
        except (json.JSONDecodeError, KeyError, OSError, TypeError, ValueError):
            path.unlink(missing_ok=True)
            return False

    def _write_notfound(self, cache_key: str, dates_tried: int):
        """å†™å…¥ notfound æ ‡è®°ï¼ˆJSON æ ¼å¼ï¼‰"""
        path = self.notfound_dir / f"{cache_key}.json"
        data = {
            "timestamp": int(time.time()),
            "dates_tried": dates_tried,
            "date_list_hash": self._get_date_list_hash(),
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f)
        except OSError as e:
            logger.warning(f"å†™å…¥ notfound æ ‡è®°å¤±è´¥: {e}")

    def _save_image_atomic(self, cache_key: str, data: bytes) -> str:
        """åŸå­å†™å…¥ç¼“å­˜å›¾ç‰‡ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶å† replace"""
        target = self.cache_dir / f"{cache_key}.png"
        tmp_path = self.cache_dir / f"{cache_key}.tmp"
        try:
            with open(tmp_path, "wb") as f:
                f.write(data)
            os.replace(tmp_path, target)
            return str(target)
        except OSError as e:
            logger.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                tmp_path.unlink(missing_ok=True)
            except OSError:
                pass
            raise

    def _get_date_list_hash(self) -> str:
        """è·å–å½“å‰æ—¥æœŸåˆ—è¡¨çš„ hash"""
        return hashlib.md5(",".join(self.date_list).encode()).hexdigest()[:8]

    def _load_date_list(self):
        """åŠ è½½æ—¥æœŸåˆ—è¡¨ï¼šåˆå¹¶è¿œç¨‹ç¼“å­˜ + ç¡¬ç¼–ç  + extra_datesï¼Œå»é‡åæŒ‰æ—¥æœŸå€’åº"""
        dates = set(HARDCODED_DATES)

        # ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        if self.dates_cache_path.exists():
            try:
                with open(self.dates_cache_path, encoding="utf-8") as f:
                    cached = json.load(f)
                if isinstance(cached, list):
                    dates.update(cached)
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"æ—¥æœŸç¼“å­˜åŠ è½½å¤±è´¥: {e}")

        # åˆå¹¶ extra_dates é…ç½®
        extra = self._get_config("extra_dates", "")
        if extra:
            for line in extra.strip().splitlines():
                line = line.strip()
                if line and line.isdigit() and len(line) == 8:
                    dates.add(line)

        self.date_list = sorted(dates, reverse=True)

    def _load_metadata_index(self):
        """ä»æœ¬åœ°ç¼“å­˜çš„å…ƒæ•°æ® JSON æ–‡ä»¶åŠ è½½ç´¢å¼•åˆ°å†…å­˜"""
        self.metadata_index.clear()
        if not self.metadata_dir.exists():
            return
        for json_file in self.metadata_dir.glob("*.json"):
            try:
                cp = json_file.stem  # æ–‡ä»¶åå°±æ˜¯ emoji codepointï¼Œå¦‚ "1f437"
                with open(json_file, encoding="utf-8") as f:
                    data = json.load(f)
                index_entry = _parse_combinations(data)
                if index_entry:
                    self.metadata_index[cp] = index_entry
            except (json.JSONDecodeError, OSError, TypeError, KeyError, AttributeError) as e:
                logger.warning(f"åŠ è½½å…ƒæ•°æ®ç´¢å¼•å¤±è´¥ {json_file.name}: {e}")

    def _lookup_date(self, cp1: str, cp2: str) -> str | None:
        """ä»å…ƒæ•°æ®ç´¢å¼•ä¸­æŸ¥æ‰¾ç»„åˆå¯¹åº”çš„æ—¥æœŸï¼ŒåŒå‘æŸ¥æ‰¾"""
        # æ–¹å‘1: cp1 çš„å…ƒæ•°æ®ä¸­æŸ¥æ‰¾ cp2
        entry = self.metadata_index.get(cp1, {})
        date = entry.get(cp2)
        if date:
            return date
        # æ–¹å‘2: cp2 çš„å…ƒæ•°æ®ä¸­æŸ¥æ‰¾ cp1
        entry = self.metadata_index.get(cp2, {})
        date = entry.get(cp1)
        if date:
            return date
        return None

    async def _fetch_and_cache_metadata(self, cp: str):
        """ä» GitHub æ‹‰å–å•ä¸ª emoji çš„å…ƒæ•°æ® JSON å¹¶ç¼“å­˜ï¼Œæ›´æ–°å†…å­˜ç´¢å¼•"""
        github_proxy = self._resolve_github_proxy()
        timeout_sec = self._get_config_int("request_timeout", 10)
        raw_url = f"https://raw.githubusercontent.com/xsalazar/emoji-kitchen-backend/main/emoji/data/{cp}.json"
        if github_proxy:
            url = f"{github_proxy}/{raw_url}"
        else:
            url = raw_url

        try:
            session = await self._ensure_session()
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout_sec)) as resp:
                if resp.status != 200:
                    logger.debug(f"å…ƒæ•°æ®æ‹‰å–å¤±è´¥ {cp}: HTTP {resp.status}")
                    return
                data = await resp.json(content_type=None)

            # ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            cache_file = self.metadata_dir / f"{cp}.json"
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump(data, f)
            except OSError as e:
                logger.warning(f"å…ƒæ•°æ®ç¼“å­˜å†™å…¥å¤±è´¥ {cp}: {e}")

            # æ›´æ–°å†…å­˜ç´¢å¼•
            index_entry = _parse_combinations(data)
            if index_entry:
                self.metadata_index[cp] = index_entry

            # ä»å…ƒæ•°æ®ä¸­æå–æ—¥æœŸåˆå¹¶åˆ°æ—¥æœŸåˆ—è¡¨
            new_dates = set()
            combinations = data.get("combinations", {}) if isinstance(data, dict) else {}
            for partner_cp, combo_list in combinations.items():
                if not isinstance(combo_list, list):
                    continue
                for item in combo_list:
                    if not isinstance(item, dict):
                        continue
                    d = item.get("date", "")
                    if d and d not in self.date_list:
                        new_dates.add(d)
            if new_dates:
                date_set = set(self.date_list) | new_dates
                self.date_list = sorted(date_set, reverse=True)

        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, OSError, KeyError, TypeError, AttributeError) as e:
            logger.debug(f"å…ƒæ•°æ®æ‹‰å–å¼‚å¸¸ {cp}: {e}")

    async def _update_dates_from_remote(self):
        """ä» GitHub æ‹‰å–æ ·æœ¬æ•°æ®æå–æ—¥æœŸåˆ—è¡¨å¹¶ç¼“å­˜"""
        github_proxy = self._resolve_github_proxy()
        timeout_sec = self._get_config_int("request_timeout", 10)
        raw_url = "https://raw.githubusercontent.com/xsalazar/emoji-kitchen-backend/main/emoji/data/1f600.json"
        if github_proxy:
            url = f"{github_proxy}/{raw_url}"
        else:
            url = raw_url

        try:
            session = await self._ensure_session()

            async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout_sec)) as resp:
                if resp.status != 200:
                    logger.warning(f"è¿œç¨‹æ—¥æœŸæ‹‰å–å¤±è´¥: HTTP {resp.status}")
                    return
                data = await resp.json(content_type=None)

            # æå–æ‰€æœ‰ date å­—æ®µ
            remote_dates = set()
            if isinstance(data, dict):
                combinations = data.get("combinations", {})
                if isinstance(combinations, dict):
                    for partner_cp, combo_list in combinations.items():
                        if isinstance(combo_list, list):
                            for item in combo_list:
                                if not isinstance(item, dict):
                                    continue
                                d = item.get("date", "")
                                if d:
                                    remote_dates.add(d)

            if remote_dates:
                # åˆå¹¶åˆ°å·²çŸ¥åˆ—è¡¨å¹¶å†™å…¥ç¼“å­˜
                all_dates = set(HARDCODED_DATES) | remote_dates
                if self.dates_cache_path.exists():
                    try:
                        with open(self.dates_cache_path, encoding="utf-8") as f:
                            existing = json.load(f)
                        if isinstance(existing, list):
                            all_dates.update(existing)
                    except (json.JSONDecodeError, OSError):
                        pass

                sorted_dates = sorted(all_dates, reverse=True)
                with open(self.dates_cache_path, "w", encoding="utf-8") as f:
                    json.dump(sorted_dates, f)

                # é‡æ–°åŠ è½½
                self._load_date_list()
                logger.info(f"æ—¥æœŸåˆ—è¡¨æ›´æ–°æˆåŠŸï¼Œå…± {len(self.date_list)} ä¸ªæ—¥æœŸ")
        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, OSError, KeyError, TypeError, AttributeError) as e:
            logger.warning(f"è¿œç¨‹æ—¥æœŸæ›´æ–°å¤±è´¥: {e}")

    async def _get_lock(self, cache_key: str) -> asyncio.Lock:
        """è·å–æŒ‡å®š cache_key çš„é”ï¼Œç”¨äºå¹¶å‘è¯·æ±‚å»é‡ï¼ˆLRU æ·˜æ±°ï¼‰"""
        async with self._global_lock:
            if cache_key in self._locks:
                self._locks.move_to_end(cache_key)
                return self._locks[cache_key]
            # å…ˆæ·˜æ±°å†æ’å…¥ï¼Œç¡®ä¿å½“å‰ key ä¸ä¼šè¢«æ·˜æ±°
            while len(self._locks) >= self._MAX_LOCKS:
                evicted = False
                for key in list(self._locks.keys()):
                    if not self._locks[key].locked():
                        del self._locks[key]
                        evicted = True
                        break
                if not evicted:
                    break  # æ‰€æœ‰é”éƒ½åœ¨ä½¿ç”¨ä¸­ï¼Œå…è®¸ä¸´æ—¶è¶…å‡ºä¸Šé™
            lock = asyncio.Lock()
            self._locks[cache_key] = lock
            return lock

    def _build_urls(self, cp1: str, cp2: str, date: str) -> list[str]:
        """ä¸ºç»™å®šæ—¥æœŸæ„é€ ä¸¤ä¸ªæ–¹å‘çš„ URLï¼ˆAâ†’B å’Œ Bâ†’Aï¼‰"""
        cdn_url = self._resolve_cdn_url()
        seg1 = codepoint_to_url_segment(cp1)
        seg2 = codepoint_to_url_segment(cp2)
        return [
            f"{cdn_url}/android/keyboard/emojikitchen/{date}/{seg1}/{seg1}_{seg2}.png",
            f"{cdn_url}/android/keyboard/emojikitchen/{date}/{seg2}/{seg2}_{seg1}.png",
        ]

    async def _try_fetch_url(self, url: str) -> bytes | None:
        """å°è¯•è¯·æ±‚å•ä¸ª URLã€‚è¿”å› None ä»…è¡¨ç¤ºç¡®è®¤ 404ï¼Œå…¶ä»–å¤±è´¥å‡ raiseã€‚"""
        timeout_sec = self._get_config_int("request_timeout", 10)
        try:
            session = await self._ensure_session()

            async with session.get(
                url, timeout=aiohttp.ClientTimeout(total=timeout_sec)
            ) as resp:
                if resp.status == 404:
                    return None
                if resp.status == 429:
                    logger.warning(f"Emoji Kitchen CDN é™æµ: {url}")
                    raise RateLimitError()
                if resp.status >= 500:
                    logger.warning(f"Emoji Kitchen CDN æœåŠ¡ç«¯é”™è¯¯ {resp.status}: {url}")
                    raise aiohttp.ClientError(f"server error {resp.status}")
                if resp.status != 200:
                    logger.warning(f"Emoji Kitchen æœªé¢„æœŸçŠ¶æ€ç  {resp.status}: {url}")
                    raise aiohttp.ClientError(f"unexpected status {resp.status}")

                data = await resp.read()
                # PNG æ ¡éªŒï¼šæ£€æŸ¥ magic bytes
                if data[:4] != b'\x89PNG':
                    logger.warning(f"Emoji Kitchen å“åº”é PNG æ ¼å¼: {url}")
                    raise aiohttp.ClientError("not PNG")
                return data
        except asyncio.CancelledError:
            raise
        except (RateLimitError, aiohttp.ClientError):
            raise
        except Exception as e:
            logger.warning(f"Emoji Kitchen è¯·æ±‚å¼‚å¸¸: {url} - {e}")
            raise aiohttp.ClientError(str(e))

    async def _try_fetch_with_semaphore(self, url: str) -> bytes | None:
        """å¸¦ semaphore é™æµçš„ URL è¯·æ±‚"""
        async with self._semaphore:
            return await self._try_fetch_url(url)

    async def _try_exact_date(self, cp1: str, cp2: str, date: str, cache_key: str) -> str | None:
        """å°è¯•ç²¾ç¡®æ—¥æœŸçš„ä¸¤ä¸ªæ–¹å‘ URLï¼Œå‘½ä¸­å³è¿”å›"""
        urls = self._build_urls(cp1, cp2, date)
        tasks = [asyncio.ensure_future(self._try_fetch_with_semaphore(url)) for url in urls]
        try:
            for coro in asyncio.as_completed(tasks):
                try:
                    result = await coro
                except RateLimitError:
                    raise  # é™æµå¼‚å¸¸å‘ä¸Šä¼ æ’­ï¼Œåœæ­¢åç»­æ¢æµ‹
                except Exception:
                    continue
                if result is not None:
                    try:
                        return self._save_image_atomic(cache_key, result)
                    except OSError:
                        return None
            return None
        finally:
            # å–æ¶ˆå‰©ä½™æœªå®Œæˆçš„ä»»åŠ¡
            for t in tasks:
                if not t.done():
                    t.cancel()
            # ç­‰å¾…å–æ¶ˆå®Œæˆï¼Œé¿å…è­¦å‘Š
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _fetch_emoji_image(self, cp1: str, cp2: str) -> str | None:
        """å°è¯•ä» CDN è·å–åˆæˆå›¾ç‰‡ï¼Œè¿”å›ç¼“å­˜è·¯å¾„æˆ– None

        ç­–ç•¥ï¼ˆä¸‰é˜¶æ®µï¼‰ï¼š
        1. ç²¾ç¡®æŸ¥æ‰¾ï¼šä»å…ƒæ•°æ®ç´¢å¼•è·å–ç¡®åˆ‡æ—¥æœŸï¼Œåªè¯·æ±‚è¯¥æ—¥æœŸï¼ˆ2 ä¸ªæ–¹å‘ï¼‰
        2. æŒ‰éœ€æ‹‰å–ï¼šç´¢å¼•æœªå‘½ä¸­æ—¶ï¼Œä» GitHub æ‹‰å–å…ƒæ•°æ®å¹¶ç¼“å­˜ï¼Œå†æ¬¡æŸ¥æ‰¾
        3. å›é€€æ¢æµ‹ï¼šä»æœªå‘½ä¸­åˆ™èµ°åŸæœ‰çš„æ—¥æœŸæ¢æµ‹é€»è¾‘
        """
        cache_key = make_cache_key(cp1, cp2)

        # === é˜¶æ®µ1ï¼šç²¾ç¡®æŸ¥æ‰¾ ===
        date = self._lookup_date(cp1, cp2)
        if date:
            result = await self._try_exact_date(cp1, cp2, date, cache_key)
            if result:
                return result

        # === é˜¶æ®µ2ï¼šæŒ‰éœ€æ‹‰å–å…ƒæ•°æ® ===
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹‰å–ï¼ˆæœ¬åœ°æ²¡æœ‰ç¼“å­˜æ–‡ä»¶æˆ–æ–‡ä»¶è¿‡æœŸï¼‰
        fetched_new = False
        for cp in (cp1, cp2):
            meta_file = self.metadata_dir / f"{cp}.json"
            need_fetch = False
            if not meta_file.exists():
                need_fetch = True
            else:
                try:
                    age = time.time() - meta_file.stat().st_mtime
                    if age > 7 * 86400:  # 7 å¤©è¿‡æœŸ
                        need_fetch = True
                except OSError:
                    need_fetch = True
            if need_fetch:
                await self._fetch_and_cache_metadata(cp)
                fetched_new = True

        if fetched_new:
            date = self._lookup_date(cp1, cp2)
            if date:
                result = await self._try_exact_date(cp1, cp2, date, cache_key)
                if result:
                    return result

        # === é˜¶æ®µ3ï¼šå›é€€åˆ°æ—¥æœŸæ¢æµ‹ ===
        return await self._probe_dates(cp1, cp2, cache_key)

    async def _probe_dates(self, cp1: str, cp2: str, cache_key: str) -> str | None:
        """å›é€€ï¼šæŒ‰æ—¥æœŸåˆ—è¡¨é€æ—¥æœŸæ¢æµ‹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        max_probe = self._get_config_int("max_probe_dates", 10)
        probe_dates = self.date_list[:max_probe]
        if not probe_dates:
            logger.warning("æ—¥æœŸåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ¢æµ‹")
            return None

        all_404 = True
        has_error = False

        for date in probe_dates:
            urls = self._build_urls(cp1, cp2, date)
            tasks = [self._try_fetch_with_semaphore(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            found_data = None
            should_stop = False
            for r in results:
                if isinstance(r, RateLimitError):
                    all_404 = False
                    has_error = True
                    should_stop = True
                elif isinstance(r, Exception):
                    all_404 = False
                    has_error = True
                elif r is not None:
                    found_data = r

            if found_data:
                try:
                    path = self._save_image_atomic(cache_key, found_data)
                    return path
                except OSError:
                    return None

            if should_stop:
                break

        # notfound åˆ¤å®šé€»è¾‘
        if all_404 and not has_error and max_probe >= len(self.date_list):
            self._write_notfound(cache_key, len(probe_dates))
            logger.info(f"Emoji Kitchen ç»„åˆä¸å­˜åœ¨: {cache_key}")
        elif has_error:
            logger.info(f"Emoji Kitchen æ¢æµ‹å­˜åœ¨ç½‘ç»œé”™è¯¯ï¼Œä¸å†™ notfound: {cache_key}")
        else:
            logger.info(f"Emoji Kitchen æœªå‘½ä¸­ï¼ˆæ¢æµ‹äº† {len(probe_dates)}/{len(self.date_list)} ä¸ªæ—¥æœŸï¼‰: {cache_key}")
        return None

    @filter.event_message_type(filter.EventMessageType.ALL)
    async def on_message(self, event: AstrMessageEvent):
        """ç›‘å¬æ‰€æœ‰æ¶ˆæ¯ï¼Œæ£€æµ‹åŒ emoji å¹¶åˆæˆ"""
        # 1. è·å–çº¯æ–‡æœ¬æ¶ˆæ¯
        msg = event.message_str.strip() if event.message_str else ""
        if not msg:
            return

        # 2. æå– emoji
        emojis = EMOJI_PATTERN.findall(msg)
        if len(emojis) != 2:
            return

        # 3. éªŒè¯æ¶ˆæ¯ä»…åŒ…å«è¿™ä¸¤ä¸ª emojiï¼ˆæ— å¤šä½™å­—ç¬¦ï¼‰
        if "".join(emojis) != msg:
            return

        # 4. è½¬æ¢ codepoint å¹¶ç”Ÿæˆ cache_key
        cp1 = emoji_to_codepoint(emojis[0])
        cp2 = emoji_to_codepoint(emojis[1])
        cache_key = make_cache_key(cp1, cp2)

        # 5. æ£€æŸ¥ç¼“å­˜
        cached = self._get_cached_image(cache_key)
        if cached:
            yield event.image_result(cached)
            event.stop_event()
            return

        # 6. æ£€æŸ¥ notfound æ ‡è®°
        if self._is_notfound(cache_key):
            return

        # 7. ä½¿ç”¨å¹¶å‘é”é˜²æ­¢é‡å¤è¯·æ±‚
        lock = await self._get_lock(cache_key)
        async with lock:
            # åŒé‡æ£€æŸ¥ï¼šè·å–é”åå†æ¬¡æ£€æŸ¥ç¼“å­˜å’Œ notfound
            cached = self._get_cached_image(cache_key)
            if cached:
                yield event.image_result(cached)
                event.stop_event()
                return
            if self._is_notfound(cache_key):
                return

            # 8. ä» CDN è·å–ï¼ˆå¼‚å¸¸å…œåº•ï¼‰
            try:
                path = await self._fetch_emoji_image(cp1, cp2)
                if path:
                    yield event.image_result(path)
                    event.stop_event()
                    return
            except (aiohttp.ClientError, asyncio.TimeoutError, OSError, RateLimitError) as e:
                logger.error(f"Emoji Kitchen è·å–å¼‚å¸¸: {e}")
            except Exception as e:
                logger.exception(f"Emoji Kitchen æœªé¢„æœŸå¼‚å¸¸: {e}")

        # æœªå‘½ä¸­ï¼šä¸ yieldï¼Œä¸ stop_eventï¼Œäº‹ä»¶ç»§ç»­ä¼ æ’­

    async def terminate(self):
        """æ’ä»¶é”€æ¯ï¼šå–æ¶ˆåå°ä»»åŠ¡ã€å…³é—­ HTTP session"""
        if self._update_task and not self._update_task.done():
            self._update_task.cancel()
            try:
                await self._update_task
            except (asyncio.CancelledError, Exception):
                pass
            self._update_task = None
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
